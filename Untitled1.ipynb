{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMo8dBN5SNcfm90RElr+8zV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Irenempax/Irenempax/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set Up Google Colab Environment\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Load the S2.pkl File\n",
        "pkl_file_path = '/content/drive/My Drive/wesad/WESAD/WESAD/S2/S2.pkl'  # Update the path if needed\n",
        "with open(pkl_file_path, 'rb') as f:\n",
        "    data = pickle.load(f, encoding='latin1')  # Update to specify encoding\n",
        "\n",
        "# Step 3: Check Available Signals\n",
        "print(\"Available signal keys:\", data['signal'].keys())  # Should show 'chest' and 'wrist'\n",
        "\n",
        "# Check what is inside the 'chest' key\n",
        "chest_signals = data['signal']['chest']\n",
        "print(\"Chest signal structure:\", chest_signals.keys())  # Print keys to see available data\n",
        "\n",
        "# Step 4: Extract ECG Data from Chest Signal\n",
        "try:\n",
        "    ecg_data = chest_signals['ECG'][:, 0]  # Access ECG data from the structure\n",
        "except KeyError as e:\n",
        "    print(f\"Error extracting ECG data: {e}\")\n",
        "    ecg_data = None  # Set ecg_data to None in case of error\n",
        "\n",
        "# Check if ecg_data was successfully extracted\n",
        "if ecg_data is None:\n",
        "    print(\"Unable to extract ECG data. Check the dataset structure.\")\n",
        "else:\n",
        "    # Step 5: Define Functions to Calculate HR and RMSSD\n",
        "    def calculate_hr(ecg_data, sampling_rate=1000):\n",
        "        # Detect peaks in the ECG data\n",
        "        peaks, _ = find_peaks(ecg_data, distance=sampling_rate/2.0)  # Minimum distance between peaks\n",
        "        hr_values = np.zeros(len(peaks) - 1)  # HR values for each interval\n",
        "\n",
        "        for i in range(1, len(peaks)):\n",
        "            # Calculate the interval between peaks in seconds\n",
        "            interval = (peaks[i] - peaks[i-1]) / sampling_rate\n",
        "            hr_values[i-1] = 60 / interval  # Convert to beats per minute (BPM)\n",
        "\n",
        "        return hr_values  # Return array of HR values\n",
        "\n",
        "    def calculate_rmssd(hr_values):\n",
        "        # Calculate RMSSD from HR values\n",
        "        if len(hr_values) < 2:\n",
        "            return np.nan  # Not enough values to calculate RMSSD\n",
        "        diff = np.diff(hr_values)\n",
        "        rmssd = np.sqrt(np.mean(diff**2))\n",
        "        return rmssd\n",
        "\n",
        "    # Step 6: Calculate HR and RMSSD Features\n",
        "    hr_values = calculate_hr(ecg_data)  # Calculate HR values from ECG\n",
        "    rmssd_value = calculate_rmssd(hr_values)  # Calculate RMSSD from HR values\n",
        "\n",
        "    # Debugging: Check the output of hr_values and rmssd\n",
        "    print(\"HR Values:\", hr_values)\n",
        "    print(\"RMSSD Value:\", rmssd_value)\n",
        "\n",
        "    # Step 7: Prepare Features and Labels\n",
        "    # Create a DataFrame with features (HR and RMSSD)\n",
        "    features = pd.DataFrame({\n",
        "        'HR': hr_values,\n",
        "        'RMSSD': [rmssd_value] * len(hr_values)  # Repeat RMSSD value for all HR values\n",
        "    })\n",
        "\n",
        "    # Assuming you have labels in the data; replace this with your actual label extraction\n",
        "    labels = data['label']  # Adjust this if necessary\n",
        "    labels_df = pd.DataFrame(labels, columns=['Label'])\n",
        "\n",
        "    # Ensure the lengths of features and labels match\n",
        "    if len(features) != len(labels_df):\n",
        "        print(f\"Length mismatch: Features ({len(features)}) vs Labels ({len(labels_df)})\")\n",
        "    else:\n",
        "        # Combine features and labels\n",
        "        combined_df = pd.concat([features, labels_df], axis=1)\n",
        "\n",
        "        # Step 8: Split the Data\n",
        "        X = combined_df[['HR', 'RMSSD']]\n",
        "        y = combined_df['Label']\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "        # Check the class distribution in training and test sets\n",
        "        print(\"Class distribution in training set:\")\n",
        "        print(pd.Series(y_train).value_counts())\n",
        "        print(\"Class distribution in test set:\")\n",
        "        print(pd.Series(y_test).value_counts())\n",
        "\n",
        "        # Step 9: Train Machine Learning Model\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Step 10: Make Predictions and Evaluate the Model\n",
        "        y_pred = model.predict(X_test)\n",
        "        print(confusion_matrix(y_test, y_pred))\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Step 11: Visualize Results\n",
        "        confusion_mtx = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "iVjDUKSDp_TG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19aec06d-783c-41c5-8029-38fdbd187051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Available signal keys: dict_keys(['chest', 'wrist'])\n",
            "Chest signal structure: dict_keys(['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp'])\n",
            "HR Values: [111.11111111 117.87819253 111.73184358 ...  82.30452675  82.75862069\n",
            "  85.47008547]\n",
            "RMSSD Value: 13.213930003007187\n",
            "Length mismatch: Features (6920) vs Labels (4255300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load the S2.pkl File\n",
        "pkl_file_path = '/content/drive/My Drive/wesad/WESAD/WESAD/S2/S2.pkl'  # Update the path if needed\n",
        "with open(pkl_file_path, 'rb') as f:\n",
        "    data = pickle.load(f, encoding='latin1')\n",
        "\n",
        "# Check the structure and shape of labels\n",
        "labels = data['label']  # Adjust this if necessary\n",
        "print(\"Labels shape:\", labels.shape)\n",
        "\n",
        "# Check unique labels to understand the class distribution\n",
        "unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "label_distribution = dict(zip(unique_labels, counts))\n",
        "print(\"Label distribution:\", label_distribution)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H62A3NP_rmta",
        "outputId": "443a9ba6-57b7-4aef-cc71-24429c75ba3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels shape: (4255300,)\n",
            "Label distribution: {0: 2142701, 1: 800800, 2: 430500, 3: 253400, 4: 537599, 6: 45500, 7: 44800}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from scipy.signal import find_peaks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Step 1: Load the S2.pkl File\n",
        "pkl_file_path = '/content/drive/My Drive/wesad/WESAD/WESAD/S2/S2.pkl'  # Update the path if needed\n",
        "with open(pkl_file_path, 'rb') as f:\n",
        "    data = pickle.load(f, encoding='latin1')\n",
        "\n",
        "# Step 2: Extract the ECG signal and labels\n",
        "ecg_signal = data['signal']['chest']['ECG']\n",
        "labels = data['label']\n",
        "\n",
        "# Step 3: Filter labels to focus on 1, 2, and 3\n",
        "filtered_indices = np.isin(labels, [1, 2, 3])\n",
        "filtered_ecg = ecg_signal[filtered_indices]\n",
        "filtered_labels = labels[filtered_indices]\n",
        "\n",
        "# Ensure the ECG signal is a 1-D array\n",
        "filtered_ecg = filtered_ecg.flatten()\n",
        "\n",
        "# Check label distribution\n",
        "unique, counts = np.unique(filtered_labels, return_counts=True)\n",
        "label_distribution = dict(zip(unique, counts))\n",
        "print(\"Filtered label distribution:\", label_distribution)\n",
        "\n",
        "# Check if we have enough samples\n",
        "if len(label_distribution) < 2:\n",
        "    raise ValueError(\"Insufficient classes available in the filtered data.\")\n",
        "\n",
        "# Step 4: Calculate Heart Rate (HR)\n",
        "def calculate_hr(ecg_signal, sampling_rate=128):\n",
        "    # Find R-peaks in the ECG signal\n",
        "    peaks, _ = find_peaks(ecg_signal, distance=sampling_rate * 0.6)  # Minimum 0.6 seconds between peaks\n",
        "    rr_intervals = np.diff(peaks) / sampling_rate  # Convert sample indices to seconds\n",
        "    hr_values = 60 / rr_intervals  # Convert RR intervals to HR (bpm)\n",
        "    return hr_values\n",
        "\n",
        "# Step 5: Calculate RMSSD\n",
        "def calculate_rmssd(hr_values):\n",
        "    return np.sqrt(np.mean(np.square(np.diff(hr_values))))\n",
        "\n",
        "# Step 6: Extract HR and RMSSD features\n",
        "hr_values = calculate_hr(filtered_ecg)\n",
        "rmssd_value = calculate_rmssd(hr_values)\n",
        "\n",
        "# Create a DataFrame for features\n",
        "features_df = pd.DataFrame({'HR': hr_values, 'RMSSD': [rmssd_value] * len(hr_values)})\n",
        "\n",
        "# Ensure the labels match the length of features_df\n",
        "labels_df = filtered_labels[:len(features_df)]\n",
        "\n",
        "# Check label distribution before oversampling\n",
        "unique, counts = np.unique(labels_df, return_counts=True)\n",
        "label_distribution_before = dict(zip(unique, counts))\n",
        "print(\"Label distribution before oversampling:\", label_distribution_before)\n",
        "\n",
        "# Step 7: Apply Random Oversampling if we have at least 2 classes\n",
        "if len(unique) > 1:\n",
        "    ros = RandomOverSampler(random_state=42)\n",
        "    X_resampled, y_resampled = ros.fit_resample(features_df, labels_df)\n",
        "\n",
        "    # Check the distribution after oversampling\n",
        "    unique, counts = np.unique(y_resampled, return_counts=True)\n",
        "    print(\"Resampled label distribution:\", dict(zip(unique, counts)))\n",
        "\n",
        "    # Step 8: Train-test split with stratification\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42)\n",
        "\n",
        "    # Step 9: Train Machine Learning Model\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Step 10: Make Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Step 11: Calculate Accuracy and Confusion Matrix\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Step 12: Display Results\n",
        "    print(f'Accuracy: {accuracy:.2f}')\n",
        "    print('Confusion Matrix:')\n",
        "    print(confusion)\n",
        "\n",
        "    # Step 13: Optional - Visualize the confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(confusion, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(3)  # Adjust according to the number of classes you have\n",
        "    plt.xticks(tick_marks, ['1', '2', '3'])\n",
        "    plt.yticks(tick_marks, ['1', '2', '3'])\n",
        "\n",
        "    # Loop over data dimensions and create text annotations\n",
        "    thresh = confusion.max() / 2.\n",
        "    for i, j in np.ndindex(confusion.shape):\n",
        "        plt.text(j, i, format(confusion[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if confusion[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Insufficient diversity in labels for training.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctR0F5Vnu1n-",
        "outputId": "b706aad3-4fb7-47b5-c864-18e6298ba481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered label distribution: {1: 800800, 2: 430500, 3: 253400}\n",
            "Label distribution before oversampling: {1: 14863}\n",
            "Insufficient diversity in labels for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WQxYAYBbvwHx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}